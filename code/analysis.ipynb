{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\annap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\annap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\annap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "import random\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "tqdm.pandas()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_files = ['cf74', 'cf75', 'cf76', 'cf77', 'cf78', 'cf79']\n",
    "fn_queries = 'cfquery'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74001</td>\n",
       "      <td>Pseudomonas aeruginosa infection in cystic fib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74002</td>\n",
       "      <td>Amylase content of mixed saliva in children.SA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74003</td>\n",
       "      <td>A clinical study of the diagnosis of cystic fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74004</td>\n",
       "      <td>A methodological study of the diagnosis of cys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74005</td>\n",
       "      <td>Proteolytic activity in duodenal juice in infa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                               TEXT\n",
       "0  74001  Pseudomonas aeruginosa infection in cystic fib...\n",
       "1  74002  Amylase content of mixed saliva in children.SA...\n",
       "2  74003  A clinical study of the diagnosis of cystic fi...\n",
       "3  74004  A methodological study of the diagnosis of cys...\n",
       "4  74005  Proteolytic activity in duodenal juice in infa..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_informations(filename):\n",
    "    data = {}\n",
    "    id_actual = None\n",
    "    information = \"\"\n",
    "\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding='ansi') as file:\n",
    "            for line in file:\n",
    "                if line.startswith(\"PN\"):\n",
    "                    # Se há uma informação anterior, armazena no dicionário\n",
    "                    if id_actual and information:\n",
    "                        data[id_actual] = information\n",
    "                        information = \"\"\n",
    "\n",
    "                    # Obtém o ID a partir da segunda palavra\n",
    "                    id_actual = line.split()[1]\n",
    "                elif line.startswith((\"TI\", \"MJ\", \"MN\", \"AB\", \"EX\")) and id_actual:\n",
    "                    information += line[3:].strip()\n",
    "                    while True:\n",
    "                        next_line = file.readline()\n",
    "                        if not next_line or re.match(r'\\b[A-Z]{2}\\s', next_line):\n",
    "                            break\n",
    "                        information += \" \" + next_line.strip()\n",
    "\n",
    "            # Adiciona a última informação ao dicionário\n",
    "            if id_actual and information:\n",
    "                data[id_actual] = information\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not founded.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Dicionário para armazenar os data\n",
    "data_complete = {}\n",
    "\n",
    "# Extrai as informações de cada file\n",
    "for fn in fn_files:\n",
    "    path_file = f\"../data/{fn}\"\n",
    "    data = extract_informations(path_file)\n",
    "\n",
    "    data_complete.update(data)\n",
    "\n",
    "df_data = pd.DataFrame(list(data_complete.items()), columns=['ID', 'TEXT'])\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QN</th>\n",
       "      <th>QU</th>\n",
       "      <th>NR</th>\n",
       "      <th>RD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What are the effects of calcium on the physica...</td>\n",
       "      <td>34</td>\n",
       "      <td>[139, 1222, 151, 2211, 166, 1, 311, 1, 370, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Can one distinguish between the effects of muc...</td>\n",
       "      <td>7</td>\n",
       "      <td>[169, 1000, 434, 1001, 454, 100, 498, 1000, 49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How are salivary glycoproteins from CF patient...</td>\n",
       "      <td>43</td>\n",
       "      <td>[23, 1000, 40, 10, 139, 2122, 190, 1, 221, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>What is the lipid composition of CF respirator...</td>\n",
       "      <td>9</td>\n",
       "      <td>[503, 1, 538, 100, 539, 100, 540, 100, 553, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Is CF mucus abnormal?</td>\n",
       "      <td>131</td>\n",
       "      <td>[23, 2220, 47, 2221, 50, 1, 60, 1, 114, 11, 13...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   QN                                                 QU   NR  \\\n",
       "0   1  What are the effects of calcium on the physica...   34   \n",
       "1   2  Can one distinguish between the effects of muc...    7   \n",
       "2   3  How are salivary glycoproteins from CF patient...   43   \n",
       "3   4  What is the lipid composition of CF respirator...    9   \n",
       "4   5                              Is CF mucus abnormal?  131   \n",
       "\n",
       "                                                  RD  \n",
       "0  [139, 1222, 151, 2211, 166, 1, 311, 1, 370, 10...  \n",
       "1  [169, 1000, 434, 1001, 454, 100, 498, 1000, 49...  \n",
       "2  [23, 1000, 40, 10, 139, 2122, 190, 1, 221, 1, ...  \n",
       "3  [503, 1, 538, 100, 539, 100, 540, 100, 553, 1,...  \n",
       "4  [23, 2220, 47, 2221, 50, 1, 60, 1, 114, 11, 13...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_file_query(file_path):\n",
    " \n",
    "    data = {'QN': [], 'QU': [], 'NR': [], 'RD': []}\n",
    "\n",
    "    qn_id = None\n",
    "    qu_texto = None\n",
    "    nr_numero = None\n",
    "    rd_lista = []\n",
    "\n",
    "    with open(file_path, 'r') as arquivo:\n",
    "        for linha in arquivo:\n",
    "            if linha.startswith('RD'):\n",
    "                rd_lista = [int(x) for x in re.findall(r'\\d+', linha)]\n",
    "                while True:\n",
    "                    try:\n",
    "                        proxima_linha = next(arquivo)\n",
    "                        if proxima_linha.startswith('QN'):\n",
    "                            # Salva os dados acumulados até aqui\n",
    "                            if qn_id is not None:\n",
    "                                data['QN'].append(qn_id)\n",
    "                                data['QU'].append(qu_texto)\n",
    "                                data['NR'].append(nr_numero)\n",
    "                                data['RD'].append(rd_lista)\n",
    "                            # Reinicia as variáveis para o próximo 'QN'\n",
    "                            qn_id = int(re.search(r'\\d+', proxima_linha).group())\n",
    "                            qu_texto = None\n",
    "                            nr_numero = None\n",
    "                            rd_lista = []\n",
    "                            break\n",
    "                        rd_lista.extend([int(x) for x in re.findall(r'\\d+', proxima_linha)])\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "            elif linha.startswith('QN'):\n",
    "                qn_id = int(re.search(r'\\d+', linha).group())\n",
    "            elif linha.startswith('QU'):\n",
    "                qu_texto = linha[3:].strip()\n",
    "            elif linha.startswith('NR'):\n",
    "                nr_numero = int(re.search(r'\\d+', linha).group())\n",
    "\n",
    "    # Adiciona os últimos dados, se houver\n",
    "    if qn_id is not None:\n",
    "        data['QN'].append(qn_id)\n",
    "        data['QU'].append(qu_texto)\n",
    "        data['NR'].append(nr_numero)\n",
    "        data['RD'].append(rd_lista)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Lê o arquivo e cria o DataFrame\n",
    "df_query = read_file_query(f'../data/{fn_queries}')\n",
    "df_query.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_config = { 'convert': 'lower', # lower or upper \n",
    "              'reduce': 'steamming', # stemming or lemming  lemming\n",
    "              'stop_words': True,  # remove if true else ~remove\n",
    "              'punctuation':True, # remove if true, else ~remove\n",
    "              'number': True,\n",
    "             }\n",
    "\n",
    "def lemmatize_text_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return ' '.join([stemmer.stem(word) for word in word_tokenize(text)])\n",
    "\n",
    "def remove_stopwords_from_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def remove_punctuation_nltk(text):\n",
    "    \n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens_without_punctuation = [word for word in tokens if word not in string.punctuation]\n",
    "    \n",
    "    return ' '.join(tokens_without_punctuation)\n",
    "\n",
    "def remove_number(text):\n",
    "    return ' '.join(re.sub(r'\\w*\\d\\w*', '', word) for word in word_tokenize(text))\n",
    "\n",
    "def preprocess_english_text(text, conf:dict, n_exemple:int):\n",
    "    \n",
    "    pp_stats = {}\n",
    "\n",
    "    if conf['convert'] == 'lower':\n",
    "        print(f\"Before lower ex.: {text[n_exemple].split('.')[0]}\")\n",
    "        text = text.str.lower()\n",
    "        pp_count_words = list(map(lambda string: len(string.split()), text))\n",
    "        pp_stats['lower'] = pp_count_words\n",
    "        print(f\"-> After lower ex.:  {text[n_exemple].split('.')[0]}\")\n",
    "\n",
    "    if conf['convert'] == 'upper':\n",
    "        print(f\"Before upper ex.: {text[n_exemple].split('.')[0]}\")\n",
    "        text = text.str.upper()\n",
    "        pp_count_words = list(map(lambda string: len(string.split()), text))\n",
    "        pp_stats['upper'] = pp_count_words\n",
    "        print(f\"-> After upper ex.:  {text[n_exemple].split('.')[0]}\")\n",
    "\n",
    "    if conf['reduce'] == 'lemming':\n",
    "        print(f\"Before lemmatizer ex.: {text[n_exemple].split('.')[0]}\")\n",
    "        processed = text.apply(lemmatize_text_spacy)\n",
    "        pp_count_words = list(map(lambda string: len(string.split()), text))\n",
    "        pp_stats['lemm'] = pp_count_words\n",
    "        print(f\"-> After lemmatizer ex.: {processed[n_exemple].split('.')[0]}\")\n",
    "\n",
    "    if conf['reduce'] == 'steamming':\n",
    "        print(f\"Before stemming ex.: {text.iloc[n_exemple].split('.')[0]}\")\n",
    "        processed = text.apply(stem_text)\n",
    "        pp_count_words = processed.apply(lambda x: len(x.split()))\n",
    "        pp_stats['stem'] = pp_count_words\n",
    "        print(f\"-> After stemming ex.: {processed.iloc[n_exemple].split('.')[0]}\")\n",
    "\n",
    "    if conf['stop_words'] == True:        \n",
    "        print(f\"Before stopword ex.: {processed[n_exemple].split('.')[0]}\")\n",
    "        processed = processed.apply(remove_stopwords_from_text)\n",
    "        pp_count_words = processed.apply(lambda x: len(x.split()))\n",
    "        pp_stats['stop_words'] = pp_count_words\n",
    "        print(f\"-> After stopword ex.: {processed.iloc[n_exemple].split('.')[0]}\")\n",
    "\n",
    "    if conf['punctuation'] == True:\n",
    "        print(f\"Before remove punctuation ex.:  {processed[n_exemple].split('.')[0]}\")\n",
    "        processed = processed.apply(remove_punctuation_nltk)\n",
    "        pp_count_words = processed.apply(lambda x: len(x.split()))\n",
    "        pp_stats['punctuation'] = pp_count_words\n",
    "        print(f\"-> After remove ponctuation ex.: {processed.iloc[n_exemple].split('.')[0]}\")\n",
    "    \n",
    "    if conf['number'] == True:\n",
    "\n",
    "        print(f\"Before remove numbers ex.:  {processed[n_exemple].split('.')[0]}\")\n",
    "        processed = [remove_number(text) for text in processed]\n",
    "        pp_count_words = [len(text.split()) for text in processed]\n",
    "        pp_stats['number'] = pp_count_words\n",
    "        print(f\"-> After remove numbers ex.: {processed[n_exemple].split('.')[0]}\")\n",
    "\n",
    "    return processed, pp_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------Data: Document--------------------------------------------------\n",
      "Before lower ex.: Pseudomonas aeruginosa infection in cystic fibrosis\n",
      "-> After lower ex.:  pseudomonas aeruginosa infection in cystic fibrosis\n",
      "Before stemming ex.: pseudomonas aeruginosa infection in cystic fibrosis\n",
      "-> After stemming ex.: pseudomona aeruginosa infect in cystic fibrosi \n",
      "Before stopword ex.: pseudomona aeruginosa infect in cystic fibrosi \n",
      "-> After stopword ex.: pseudomona aeruginosa infect cystic fibrosi \n",
      "Before remove punctuation ex.:  pseudomona aeruginosa infect cystic fibrosi \n",
      "-> After remove ponctuation ex.: pseudomona aeruginosa infect cystic fibrosi occurr precipit antibodi pseudomona aeruginosa relat concentr sixteen serum protein clinic radiograph statu lungscysticfibrosi co pseudomonasaeruginosa im pseudomonasinfect co respiratorytractinfect coth signific pseudomona aeruginosa infect respiratori tract 9 cystic fibrosi patient studi mean immunoelectrophoret analysi patient sera number precipitin pseudomona aeruginosa concentr 16 serum protein addit clinic radiograph statu lung evalu use 2 score system precipitin pseudomona aeruginosa demonstr sera maximum number one serum wa 22 concentr 12 serum protein significantli chang compar match control person notabl igg iga elev acut phase protein chang latter suggest activ tissu damag concentr 3 acut phase protein notabl haptoglobin correl number precipitin suggest respiratori tract infect patient mani precipitin accompani tissu damag infect patient precipitin result indic protect valu mani precipitin tissu respiratori tract\n",
      "Before remove numbers ex.:  pseudomona aeruginosa infect cystic fibrosi occurr precipit antibodi pseudomona aeruginosa relat concentr sixteen serum protein clinic radiograph statu lungscysticfibrosi co pseudomonasaeruginosa im pseudomonasinfect co respiratorytractinfect coth signific pseudomona aeruginosa infect respiratori tract 9 cystic fibrosi patient studi mean immunoelectrophoret analysi patient sera number precipitin pseudomona aeruginosa concentr 16 serum protein addit clinic radiograph statu lung evalu use 2 score system precipitin pseudomona aeruginosa demonstr sera maximum number one serum wa 22 concentr 12 serum protein significantli chang compar match control person notabl igg iga elev acut phase protein chang latter suggest activ tissu damag concentr 3 acut phase protein notabl haptoglobin correl number precipitin suggest respiratori tract infect patient mani precipitin accompani tissu damag infect patient precipitin result indic protect valu mani precipitin tissu respiratori tract\n",
      "-> After remove numbers ex.: pseudomona aeruginosa infect cystic fibrosi occurr precipit antibodi pseudomona aeruginosa relat concentr sixteen serum protein clinic radiograph statu lungscysticfibrosi co pseudomonasaeruginosa im pseudomonasinfect co respiratorytractinfect coth signific pseudomona aeruginosa infect respiratori tract  cystic fibrosi patient studi mean immunoelectrophoret analysi patient sera number precipitin pseudomona aeruginosa concentr  serum protein addit clinic radiograph statu lung evalu use  score system precipitin pseudomona aeruginosa demonstr sera maximum number one serum wa  concentr  serum protein significantli chang compar match control person notabl igg iga elev acut phase protein chang latter suggest activ tissu damag concentr  acut phase protein notabl haptoglobin correl number precipitin suggest respiratori tract infect patient mani precipitin accompani tissu damag infect patient precipitin result indic protect valu mani precipitin tissu respiratori tract\n",
      "--------------------------------------------------Data: Queries--------------------------------------------------\n",
      "Before lower ex.: What live the effectuate of calcium along the physical properties of mucous_secretion\n",
      "-> After lower ex.:  what live the effectuate of calcium along the physical properties of mucous_secretion\n",
      "Before stemming ex.: what live the effectuate of calcium along the physical properties of mucous_secretion\n",
      "-> After stemming ex.: what live the effectu of calcium along the physic properti of mucous_secret\n",
      "Before stopword ex.: what live the effectu of calcium along the physic properti of mucous_secret\n",
      "-> After stopword ex.: live effectu calcium along physic properti mucous_secret\n",
      "Before remove punctuation ex.:  live effectu calcium along physic properti mucous_secret\n",
      "-> After remove ponctuation ex.: live effectu calcium along physic properti mucoussecret\n",
      "Before remove numbers ex.:  live effectu calcium along physic properti mucoussecret\n",
      "-> After remove numbers ex.: live effectu calcium along physic properti mucoussecret\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'-'*50}Data: Document{'-'*50}\")\n",
    "df_data_pp = df_data.copy()\n",
    "df_data_pp['pp'], pp_count_d= preprocess_english_text(df_data['TEXT'], pp_config, n_exemple=0)\n",
    "\n",
    "print(f\"{'-'*50}Data: Queries{'-'*50}\")\n",
    "df_query_pp = df_query.copy()\n",
    "df_query_pp['pp'], pp_count_q= preprocess_english_text(df_query['QU'], pp_config, n_exemple=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(data_text, query_text):\n",
    "    \n",
    "    data_complete = data_text.to_list()\n",
    "    data_complete.extend(query_text.to_list())\n",
    "\n",
    "    words = []\n",
    "\n",
    "    for phrase in data_complete:\n",
    "        for word in phrase.split():\n",
    "            words.append(word)\n",
    "\n",
    "\n",
    "    print(\"Número total de palavras:\", len(set(words)))\n",
    "    return set(words)\n",
    "\n",
    "def get_synonyms(word):\n",
    "\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "def choose_reference_synonym(synonyms):\n",
    "    return random.choice(synonyms) if synonyms else None\n",
    "\n",
    "def replace_synonyms_in_text(text, synonyms_dict):\n",
    "\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()\n",
    "        replaced_words = [synonyms_dict.get(word, word) for word in words]\n",
    "        return ' '.join(replaced_words)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def reduce_vocabulary(data, query, col_text):\n",
    " \n",
    "    vocabulary = get_vocabulary(data[col_text], query[col_text])\n",
    "    print(\"Len vocabulary initial:\", len(vocabulary))\n",
    "    \n",
    "    synonyms_uniques = {}  \n",
    "\n",
    "    for word in vocabulary:\n",
    "        synonyms = get_synonyms(word)\n",
    "        word_reference = choose_reference_synonym(synonyms)\n",
    "        if word_reference:\n",
    "            for synonym in synonyms:\n",
    "                synonyms_uniques[synonym] = word_reference\n",
    "\n",
    "    print(f\"Before synonyms ex.: {data.loc[0, col_text]}\")\n",
    "\n",
    "    # Aplicando a substituição de sinônimos\n",
    "    r_data = data.copy()\n",
    "    r_query = query.copy()\n",
    "    r_data[col_text] = data[col_text].apply(lambda text: replace_synonyms_in_text(text, synonyms_uniques))\n",
    "    r_query[col_text] = query[col_text].apply(lambda text: replace_synonyms_in_text(text, synonyms_uniques))\n",
    "\n",
    "    print(f\"After synonyms ex.: {data.loc[0, col_text]}\")\n",
    "\n",
    "    vocabulary = get_vocabulary(r_data[col_text], r_query[col_text])\n",
    "    print(\"Len vocabulary final:\", len(vocabulary))\n",
    "   \n",
    "    return r_data, r_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de palavras: 9064\n",
      "Len vocabulary initial: 9064\n",
      "Before synonyms ex.: pseudomona aeruginosa infect cystic fibrosi occurr precipit antibodi pseudomona aeruginosa relat concentr sixteen serum protein clinic radiograph statu lungscysticfibrosi co pseudomonasaeruginosa im pseudomonasinfect co respiratorytractinfect coth signific pseudomona aeruginosa infect respiratori tract  cystic fibrosi patient studi mean immunoelectrophoret analysi patient sera number precipitin pseudomona aeruginosa concentr  serum protein addit clinic radiograph statu lung evalu use  score system precipitin pseudomona aeruginosa demonstr sera maximum number one serum wa  concentr  serum protein significantli chang compar match control person notabl igg iga elev acut phase protein chang latter suggest activ tissu damag concentr  acut phase protein notabl haptoglobin correl number precipitin suggest respiratori tract infect patient mani precipitin accompani tissu damag infect patient precipitin result indic protect valu mani precipitin tissu respiratori tract\n",
      "After synonyms ex.: pseudomona aeruginosa infect cystic fibrosi occurr precipit antibodi pseudomona aeruginosa relat concentr sixteen serum protein clinic radiograph statu lungscysticfibrosi co pseudomonasaeruginosa im pseudomonasinfect co respiratorytractinfect coth signific pseudomona aeruginosa infect respiratori tract  cystic fibrosi patient studi mean immunoelectrophoret analysi patient sera number precipitin pseudomona aeruginosa concentr  serum protein addit clinic radiograph statu lung evalu use  score system precipitin pseudomona aeruginosa demonstr sera maximum number one serum wa  concentr  serum protein significantli chang compar match control person notabl igg iga elev acut phase protein chang latter suggest activ tissu damag concentr  acut phase protein notabl haptoglobin correl number precipitin suggest respiratori tract infect patient mani precipitin accompani tissu damag infect patient precipitin result indic protect valu mani precipitin tissu respiratori tract\n",
      "Número total de palavras: 8395\n",
      "Len vocabulary final: 8395\n"
     ]
    }
   ],
   "source": [
    "# Involve both datas\n",
    "df_dr, df_qr = reduce_vocabulary(df_data_pp, df_query_pp, 'pp')\n",
    "\n",
    "# Involve both data, but random choose the synonims\n",
    "#data, query = reduce_vocabulary(df_data, df_query, ['TEXT', 'QU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74001</td>\n",
       "      <td>Pseudomonas aeruginosa infection in cystic fib...</td>\n",
       "      <td>pseudomona aeruginosa infect cystic fibrosi oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74002</td>\n",
       "      <td>Amylase content of mixed saliva in children.SA...</td>\n",
       "      <td>amylas content mix saliva childrensaliva en am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74003</td>\n",
       "      <td>A clinical study of the diagnosis of cystic fi...</td>\n",
       "      <td>clinic studi diagnosi cystic fibrosi instrumen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74004</td>\n",
       "      <td>A methodological study of the diagnosis of cys...</td>\n",
       "      <td>methodolog studi diagnosi cystic fibrosi instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74005</td>\n",
       "      <td>Proteolytic activity in duodenal juice in infa...</td>\n",
       "      <td>proteolyt activ duoden juic infant children ad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                               TEXT  \\\n",
       "0  74001  Pseudomonas aeruginosa infection in cystic fib...   \n",
       "1  74002  Amylase content of mixed saliva in children.SA...   \n",
       "2  74003  A clinical study of the diagnosis of cystic fi...   \n",
       "3  74004  A methodological study of the diagnosis of cys...   \n",
       "4  74005  Proteolytic activity in duodenal juice in infa...   \n",
       "\n",
       "                                                  pp  \n",
       "0  pseudomona aeruginosa infect cystic fibrosi oc...  \n",
       "1  amylas content mix saliva childrensaliva en am...  \n",
       "2  clinic studi diagnosi cystic fibrosi instrumen...  \n",
       "3  methodolog studi diagnosi cystic fibrosi instr...  \n",
       "4  proteolyt activ duoden juic infant children ad...  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74001</td>\n",
       "      <td>Pseudomonas aeruginosa infection in cystic fib...</td>\n",
       "      <td>pseudomona aeruginosa infect cystic fibrosi oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74002</td>\n",
       "      <td>Amylase content of mixed saliva in children.SA...</td>\n",
       "      <td>amylas case mix tongue childrensaliva nut amyl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74003</td>\n",
       "      <td>A clinical study of the diagnosis of cystic fi...</td>\n",
       "      <td>clinic studi diagnosi cystic fibrosi tool_arou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74004</td>\n",
       "      <td>A methodological study of the diagnosis of cys...</td>\n",
       "      <td>methodolog studi diagnosi cystic fibrosi tool_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74005</td>\n",
       "      <td>Proteolytic activity in duodenal juice in infa...</td>\n",
       "      <td>proteolyt activ duoden juic infant children ad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                               TEXT  \\\n",
       "0  74001  Pseudomonas aeruginosa infection in cystic fib...   \n",
       "1  74002  Amylase content of mixed saliva in children.SA...   \n",
       "2  74003  A clinical study of the diagnosis of cystic fi...   \n",
       "3  74004  A methodological study of the diagnosis of cys...   \n",
       "4  74005  Proteolytic activity in duodenal juice in infa...   \n",
       "\n",
       "                                                  pp  \n",
       "0  pseudomona aeruginosa infect cystic fibrosi oc...  \n",
       "1  amylas case mix tongue childrensaliva nut amyl...  \n",
       "2  clinic studi diagnosi cystic fibrosi tool_arou...  \n",
       "3  methodolog studi diagnosi cystic fibrosi tool_...  \n",
       "4  proteolyt activ duoden juic infant children ad...  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "958309"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74001</td>\n",
       "      <td>Pseudomonas aeruginosa infection in cystic fib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74002</td>\n",
       "      <td>Amylase content of mixed saliva in children.SA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74003</td>\n",
       "      <td>A clinical study of the diagnosis of cystic fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74004</td>\n",
       "      <td>A methodological study of the diagnosis of cys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74005</td>\n",
       "      <td>Proteolytic activity in duodenal juice in infa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                               TEXT\n",
       "0  74001  Pseudomonas aeruginosa infection in cystic fib...\n",
       "1  74002  Amylase content of mixed saliva in children.SA...\n",
       "2  74003  A clinical study of the diagnosis of cystic fi...\n",
       "3  74004  A methodological study of the diagnosis of cys...\n",
       "4  74005  Proteolytic activity in duodenal juice in infa..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_technicals(data_documents:dict, data_queries:pd.DataFrame, type:str):\n",
    "\n",
    "    if type == 'tf':\n",
    "        vectorizer_tf = TfidfVectorizer(use_idf=False, norm='l1') \n",
    "    elif type == 'idf':\n",
    "        vectorizer_idf = TfidfVectorizer(use_idf=True, smooth_idf=False, norm='l2')  # norm='l2' para obter o IDF\n",
    "    elif type == 'tf-idf':\n",
    "        # Inicialize o vetorizador TF-IDF\n",
    "        vectorizer = TfidfVectorizer()\n",
    "\n",
    "    document_matrix = vectorizer.fit_transform(data_documents)\n",
    "    query_vector = vectorizer.transform(data_queries)\n",
    "\n",
    "    # Obtenha os nomes das features (words)\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Crie DataFrames para os vetores TF-IDF\n",
    "    df_documents = pd.DataFrame(data=document_matrix.toarray(), columns=words)\n",
    "    df_queries = pd.DataFrame(data=query_vector.toarray(), columns=words)\n",
    "\n",
    "    return df_documents, df_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_documents, df_queries =  tf_idf_technicals(data_preprocessed.values(), df_queries['QU_preprocessed'], 'tf-idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aat</th>\n",
       "      <th>aathe</th>\n",
       "      <th>aathis</th>\n",
       "      <th>ab</th>\n",
       "      <th>abalthough</th>\n",
       "      <th>aban</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>...</th>\n",
       "      <th>zeta</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zn</th>\n",
       "      <th>zona</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zymogen</th>\n",
       "      <th>zymogengranule</th>\n",
       "      <th>zymograms</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12183 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aat  aathe  aathis   ab  abalthough  aban  abandon  abandoned  \\\n",
       "0  0.0  0.0    0.0     0.0  0.0         0.0   0.0      0.0        0.0   \n",
       "1  0.0  0.0    0.0     0.0  0.0         0.0   0.0      0.0        0.0   \n",
       "2  0.0  0.0    0.0     0.0  0.0         0.0   0.0      0.0        0.0   \n",
       "3  0.0  0.0    0.0     0.0  0.0         0.0   0.0      0.0        0.0   \n",
       "4  0.0  0.0    0.0     0.0  0.0         0.0   0.0      0.0        0.0   \n",
       "\n",
       "   abdomen  ...  zeta  zinc   zn  zona  zone  zones  zymogen  zymogengranule  \\\n",
       "0      0.0  ...   0.0   0.0  0.0   0.0   0.0    0.0      0.0             0.0   \n",
       "1      0.0  ...   0.0   0.0  0.0   0.0   0.0    0.0      0.0             0.0   \n",
       "2      0.0  ...   0.0   0.0  0.0   0.0   0.0    0.0      0.0             0.0   \n",
       "3      0.0  ...   0.0   0.0  0.0   0.0   0.0    0.0      0.0             0.0   \n",
       "4      0.0  ...   0.0   0.0  0.0   0.0   0.0    0.0      0.0             0.0   \n",
       "\n",
       "   zymograms   zz  \n",
       "0        0.0  0.0  \n",
       "1        0.0  0.0  \n",
       "2        0.0  0.0  \n",
       "3        0.0  0.0  \n",
       "4        0.0  0.0  \n",
       "\n",
       "[5 rows x 12183 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aat</th>\n",
       "      <th>aathe</th>\n",
       "      <th>aathis</th>\n",
       "      <th>ab</th>\n",
       "      <th>abalthough</th>\n",
       "      <th>aban</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>...</th>\n",
       "      <th>zeta</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zn</th>\n",
       "      <th>zona</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zymogen</th>\n",
       "      <th>zymogengranule</th>\n",
       "      <th>zymograms</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12183 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aat  aathe  aathis   ab  abalthough  aban  abandon  abandoned  \\\n",
       "0  0.0  0.0    0.0     0.0  0.0         0.0   0.0      0.0        0.0   \n",
       "1  0.0  0.0    0.0     0.0  0.0         0.0   0.0      0.0        0.0   \n",
       "2  0.0  0.0    0.0     0.0  0.0         0.0   0.0      0.0        0.0   \n",
       "3  0.0  0.0    0.0     0.0  0.0         0.0   0.0      0.0        0.0   \n",
       "4  0.0  0.0    0.0     0.0  0.0         0.0   0.0      0.0        0.0   \n",
       "\n",
       "   abdomen  ...  zeta  zinc   zn  zona  zone  zones  zymogen  zymogengranule  \\\n",
       "0      0.0  ...   0.0   0.0  0.0   0.0   0.0    0.0      0.0             0.0   \n",
       "1      0.0  ...   0.0   0.0  0.0   0.0   0.0    0.0      0.0             0.0   \n",
       "2      0.0  ...   0.0   0.0  0.0   0.0   0.0    0.0      0.0             0.0   \n",
       "3      0.0  ...   0.0   0.0  0.0   0.0   0.0    0.0      0.0             0.0   \n",
       "4      0.0  ...   0.0   0.0  0.0   0.0   0.0    0.0      0.0             0.0   \n",
       "\n",
       "   zymograms   zz  \n",
       "0        0.0  0.0  \n",
       "1        0.0  0.0  \n",
       "2        0.0  0.0  \n",
       "3        0.0  0.0  \n",
       "4        0.0  0.0  \n",
       "\n",
       "[5 rows x 12183 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetores TF-IDF dos documentos:\n",
      "   documento      este  primeiro   segundo  terceiro\n",
      "0   0.453295  0.453295  0.767495  0.000000  0.000000\n",
      "1   0.713070  0.356535  0.000000  0.603667  0.000000\n",
      "2   0.453295  0.453295  0.000000  0.000000  0.767495\n",
      "\n",
      "Vetores TF-IDF das consultas:\n",
      "   documento  este  primeiro  segundo  terceiro\n",
      "0        0.0   0.0       0.0      0.0       0.0\n",
      "1        0.0   0.0       0.0      0.0       0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Exemplo de documentos e consultas\n",
    "documentos = [\"Este é o primeiro documento.\", \"Este documento é o segundo documento.\", \"E este é o terceiro documento.\"]\n",
    "consultas = [\"Esta é uma consulta.\", \"Outra consulta aqui.\"]\n",
    "\n",
    "# Inicialize o vetorizador TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajuste o vetorizador aos documentos\n",
    "tfidf_matrix = vectorizer.fit_transform(documentos)\n",
    "\n",
    "# Transforme as consultas em vetores TF-IDF\n",
    "vetores_consultas = vectorizer.transform(consultas)\n",
    "\n",
    "# Obtenha os nomes das features (palavras)\n",
    "palavras = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Crie DataFrames para os vetores TF-IDF\n",
    "df_documentos = pd.DataFrame(data=tfidf_matrix.toarray(), columns=palavras)\n",
    "df_consultas = pd.DataFrame(data=vetores_consultas.toarray(), columns=palavras)\n",
    "\n",
    "print(\"Vetores TF-IDF dos documentos:\")\n",
    "print(df_documentos)\n",
    "\n",
    "print(\"\\nVetores TF-IDF das consultas:\")\n",
    "print(df_consultas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.TfidfVectorizer"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "information_retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
